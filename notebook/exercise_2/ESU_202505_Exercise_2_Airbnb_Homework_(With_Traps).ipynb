{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0HI2XzlYo28"
   },
   "source": [
    "# üè† Homework: Predicting Airbnb Listing Prices\n",
    "\n",
    "**Your mission:** Clean and analyze a real-world Airbnb dataset, and build a model to predict listing prices.\n",
    "\n",
    "This dataset contains dirty, messy, and surprising data ‚Äî just like real life. Your job is to:\n",
    "\n",
    "1. **Explore and clean the data**\n",
    "2. **Engineer meaningful features**\n",
    "3. **Split into train/test sets**\n",
    "4. **Build and evaluate a linear regression model**\n",
    "5. **Interpret your results**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWdqz8XqYo29"
   },
   "source": [
    "## üîç Part A: Load and Explore\n",
    "\n",
    "1. Load the CSV file `airbnb_dirty_mock.csv`\n",
    "2. View column names, data types, and general structure\n",
    "3. Plot a histogram of `price`\n",
    "4. What issues or red flags do you immediately notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "rLau7fKdYo2-",
    "outputId": "566cd53f-f900-4ca7-9cf7-870a8c4b62ba"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pgeocode\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# This is to avoid the SettingWithCopyWarning in pandas\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_url = 'https://www.dropbox.com/scl/fi/gvg9vt3lcnh1807q0enlm/airbnb_dirty.csv?rlkey=x2q13wifq69djvu85zmsof530&dl=1'\n",
    "csv_file = 'airbnb_dirty.csv'\n",
    "\n",
    "if not os.path.exists(csv_file):\n",
    "    !wget -O \"$csv_file\" \"$csv_url\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns, df.shape)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_series = df['price'].str.extract(r'(\\d+\\.?\\d*)', expand=False).astype(float).dropna()\n",
    "\n",
    "# --- Plotting a Box Plot to Visualize Outliers ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.boxplot(price_series, vert=False)\n",
    "plt.title('Box Plot of Airbnb Prices')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.yticks([]) # Hide y-axis ticks for clarity\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.savefig('price_boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p9999 = price_series.quantile(0.9999)\n",
    "capped_prices = price_series[price_series <= p9999]\n",
    "\n",
    "# Use a standard rule (e.g., Sturges' rule) for a reasonable number of bins\n",
    "# num_bins_capped = int(np.ceil(1 + np.log2(len(capped_prices))))\n",
    "\n",
    "\n",
    "# --- Plotting a More Informative Capped Histogram ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(capped_prices, bins=10, edgecolor='black')\n",
    "plt.title(f'Distribution of Airbnb Prices (Capped at 99th percentile: ${p9999:.2f})')\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add lines for mean and median to show skewness\n",
    "plt.axvline(price_series.median(), color='red', linestyle='dashed', linewidth=2, label=f'Median Price: ${price_series.median():.2f}')\n",
    "plt.axvline(price_series.mean(), color='green', linestyle='dashed', linewidth=2, label=f'Mean Price: ${price_series.mean():.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig('price_histogram_capped.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq0BZ2_6Yo2-"
   },
   "source": [
    "## üßπ Part B: Data Cleaning\n",
    "\n",
    "**Clean your dataset to make it ready for modeling.** Consider the following steps:\n",
    "- Fix or convert `price` to numeric\n",
    "- Convert `last_review` to datetime (hint: use `errors='coerce'`)\n",
    "- Handle invalid or missing data in `reviews_per_month`\n",
    "- Drop columns that are not useful (e.g., `host_name`, `host_phone`, or duplicates)\n",
    "- Handle duplicate rows\n",
    "- Make sure categorical values are consistent (e.g. whitespace in `room_type`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \" USD\" and \"$\" using a regular expression\n",
    "df['price'] = df['price'].str.replace(' USD|\\\\$', '', regex=True)\n",
    "\n",
    "# Convert to numeric, raise error if unable to convert values\n",
    "df['price'] = pd.to_numeric(df['price'], errors='raise')\n",
    "print(f\"{len(df['price'][df['price'] < p9999].dropna())}\")\n",
    "df = df[df['price'] < p9999].dropna(subset=['price'])\n",
    "\n",
    "display(df[[ 'price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_rows(df1, df2):\n",
    "    \"\"\"\n",
    "    Finds rows that exist in df1 but not in df2.\n",
    "    This comparison is based on all columns.\n",
    "    \"\"\"\n",
    "    # An outer merge aligns the two DataFrames. The indicator shows the source of each row.\n",
    "    merged_df = df1.merge(df2, how='outer', indicator=True)\n",
    "    \n",
    "    # Filter for rows that are exclusive to the left DataFrame (df1).\n",
    "    missing_rows_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    print(f\"Missing rows columns: {missing_rows_df.columns}, shape: {missing_rows_df.shape}\")\n",
    "    return missing_rows_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names of any whitespace\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Remove columns, then rows that are not useful\n",
    "# This will improve performance of operations on large datasets\n",
    "df = df.drop(columns=['name', 'host_name', 'host_phone'])\n",
    "\n",
    "df_before = df\n",
    "\n",
    "# Remove latter occurrence of rows with duplicates in the 'id' column\n",
    "df = df.drop_duplicates(subset=['id'])\n",
    "\n",
    "display(find_missing_rows(df_before, df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime and then to initial Unix timestamps (int64).\n",
    "# Invalid dates become the integer -9223372037.\n",
    "temp_series_int = pd.to_datetime(df['last_review'], errors='coerce').astype(np.int64) // 10**9\n",
    "\n",
    "# To calculate the median correctly, make a float copy and replace the NaT integer with np.nan.\n",
    "temp_series_for_median = temp_series_int.replace({-9223372037: np.nan})\n",
    "# This calculates the median as a float.\n",
    "median_timestamp = temp_series_for_median.median() \n",
    "# Go back to the original integer series and replace the NaT placeholder.\n",
    "# Convert the calculated float median to an integer before replacing.\n",
    "df['last_review'] = temp_series_int.replace({-9223372037: int(median_timestamp)})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'room_type' column by removing leading/trailing whitespace and converting to lowercase\n",
    "df['room_type'] = df['room_type'].str.strip().str.lower()\n",
    "# Check the unique values in 'room_type' to ensure consistency\n",
    "df['room_type'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "def convert_zip_to_city(value, nomi):\n",
    "    \"\"\"\n",
    "    Conditionally converts a zip code to a city name.\n",
    "    - If the value is not a 5-digit number, it's returned as is.\n",
    "    - If the value is a zip code, it's converted to a city name.\n",
    "    \"\"\"\n",
    "    # Convert value to string to handle potential mixed types\n",
    "    s_value = str(value).strip()\n",
    "\n",
    "    # Check if the value is a 5-digit number (a likely zip code)\n",
    "    if s_value.isdigit() and len(s_value) == 5:\n",
    "        try:\n",
    "            # Query for the location data of the zip code\n",
    "            location_info = nomi.query_postal_code(s_value)\n",
    "            \n",
    "            # pgeocode returns NaN for invalid codes, so check for that\n",
    "            if pd.isna(location_info.place_name):\n",
    "                return value # Return original value if zip is not found\n",
    "\n",
    "            city = location_info['place_name']\n",
    "            zip_code = location_info['postal_code']\n",
    "\n",
    "           \n",
    "            # Conditional replacement for 'New York'\n",
    "            if city == 'New York':\n",
    "                # TODO: handle more cities with alternative names\n",
    "                alternative_city_name = 'Manhattan'\n",
    "                print(f\"Converting zip code {zip_code} to city: {alternative_city_name}\")\n",
    "                return alternative_city_name\n",
    "            elif city == 'Long Island City':\n",
    "                alternative_city_name = 'Queens'\n",
    "                print(f\"Converting zip code {zip_code} to city: {alternative_city_name}\")\n",
    "                return alternative_city_name\n",
    "            else:\n",
    "                print(f\"Converting zip code {zip_code} to city: {city}\")\n",
    "                return city\n",
    "        except Exception:\n",
    "            # If any error occurs during lookup, return the original value\n",
    "            return value\n",
    "    else:\n",
    "        # If it's not a 5-digit number (e.g., \"Queens\", \"Brooklyn\"), return it unchanged\n",
    "        return value\n",
    "\n",
    "# Apply the updated function to the 'neighbourhood_group' column\n",
    "df['neighbourhood_group'] = df['neighbourhood_group'].apply(lambda x: convert_zip_to_city(x, nomi))\n",
    "\n",
    "df['neighbourhood_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBdDKx0kYo2-"
   },
   "source": [
    "## ‚öôÔ∏è Part C: Feature Engineering\n",
    "\n",
    "1. Create `log_price` as the log of the `price` column (use `np.log1p`)\n",
    "2. One-hot encode `room_type` and `neighbourhood_group`\n",
    "3. Create a new feature: `reviews_x_freq = number_of_reviews * reviews_per_month`\n",
    "4. Optionally: Calculate days since `last_review`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_wifi'] = df['has_wifi'].astype(str).str.lower().isin(['true', 't', '1']).astype(int)\n",
    "display(df[['id', 'has_wifi']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'log_price' that contains the natural logarithm of 'price'\n",
    "df['log_price'] = df['price'].apply(lambda x: np.log1p(x))\n",
    "df = df.drop(['price'], axis=1)\n",
    "display(df[['id', 'log_price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert reviews_per_month to numeric, coerce errors, then drop NaN values\n",
    "df['reviews_per_month'] = pd.to_numeric(df['reviews_per_month'], errors='coerce').copy()\n",
    "df = df.dropna(subset=['reviews_per_month'])\n",
    "print(len(df['reviews_per_month']))\n",
    "\n",
    "# create interaction feature. `reviews_x_freq`\n",
    "df['reviews_x_freq'] = df['number_of_reviews'] * df['reviews_per_month']\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_wifi'] = df['has_wifi'].astype(str).str.lower().isin(['true', 't', '1']).astype(int)\n",
    "display(df[['id', 'has_wifi']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# Handle categorical features within the loop for each feature set\n",
    "X = df.drop(columns=['id', 'log_price']).copy()\n",
    "feature_set = X.columns.tolist()\n",
    "\n",
    "# Identify categorical and numerical features for the current set\n",
    "numerical_features = [column for column in X.columns if np.issubdtype(X[column].dtype, np.number)]\n",
    "# Identify categorical features.\n",
    "categorical_features = [column for column in X.columns if X[column].dtype in ['object', 'category']]\n",
    "display(f\"Numerical features: {numerical_features}\")\n",
    "display(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "\n",
    "\n",
    "# Create the column transformer for one-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "\t\ttransformers=[\n",
    "\t\t\t\t('num', 'passthrough', numerical_features),\n",
    "\t\t\t\t('cat', OneHotEncoder(handle_unknown='error'), categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sBVW14YYo2_"
   },
   "source": [
    "## ‚úÇÔ∏è Part D: Train/Test Split\n",
    "\n",
    "Use `train_test_split` to divide your data:\n",
    "- 80% training\n",
    "- 20% testing\n",
    "\n",
    "Select relevant features for modeling (no ID, name, or original price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p50g5xHFYo2_"
   },
   "source": [
    "## üìà Part E: Linear Regression Modeling\n",
    "\n",
    "1. Train a `LinearRegression` model to predict `log_price`\n",
    "2. Evaluate it on both train and test sets using:\n",
    "   - RMSE (Root Mean Squared Error)\n",
    "   - R¬≤ (coefficient of determination)\n",
    "3. Print model coefficients\n",
    "4. Interpret: Which features have the strongest impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model pipeline\n",
    "# Add a normalizer\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t# ('scaler', MinMaxScaler()),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t('regressor', LinearRegression())])\n",
    "y = df['log_price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "display(f\"R2 score: {r2:.4f}\")\n",
    "display(f\"RMSE: {root_mean_squared_error(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Fixed features shape: {X.shape}\")\n",
    "print(f\"Remaining features: {X.columns.tolist()}\")\n",
    "\n",
    "# Add days_since_last_review feature\n",
    "current_timestamp = int(time.time())\n",
    "df['days_since_last_review'] = (current_timestamp - df['last_review']) / 86400  # Convert seconds to days\n",
    "\n",
    "print(f\"Days since last review - Min: {df['days_since_last_review'].min():.1f}, Max: {df['days_since_last_review'].max():.1f}\")\n",
    "\n",
    "# Update feature sets after adding new feature\n",
    "X = df.drop(columns=['log_price', 'id']).copy()\n",
    "numerical_features = [column for column in X.columns if np.issubdtype(X[column].dtype, np.number)]\n",
    "categorical_features = [column for column in X.columns if X[column].dtype in ['object', 'category']]\n",
    "\n",
    "print(f\"Updated numerical features: {numerical_features}\")\n",
    "print(f\"Updated categorical features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED PIPELINE: Remove MinMaxScaler and use corrected features\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Create the corrected preprocessor with updated feature lists\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='error'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the corrected pipeline without MinMaxScaler\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', LinearRegression())])\n",
    "\n",
    "y = df['log_price']\n",
    "\n",
    "# Split the data into training and testing sets with corrected X\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cross-validation for robust evaluation\n",
    "scoring = {\n",
    "    'RMSE': 'neg_root_mean_squared_error',\n",
    "    'R2': 'r2'\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(pipeline, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "# The RMSE scores are negative, so we multiply by -1 to make them positive\n",
    "mean_cv_rmse = -np.mean(cv_results['test_RMSE'])\n",
    "mean_cv_r2 = np.mean(cv_results['test_R2'])\n",
    "\n",
    "print(f\"Mean CV RMSE: {mean_cv_rmse:.4f}\")\n",
    "print(f\"Mean CV R-squared: {mean_cv_r2:.4f}\")\n",
    "\n",
    "# Fit the pipeline on the entire training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate and print the performance metrics for the test set\n",
    "test_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"--- Test Set Performance ---\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"Test R-squared: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Coefficient Analysis\n",
    "# Get the one-hot encoder from the pipeline's preprocessor step\n",
    "one_hot_encoder = pipeline.named_steps['preprocessor'].named_transformers_['cat']\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "encoded_feature_names = one_hot_encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine the original numerical feature names with the new encoded categorical names\n",
    "all_feature_names = numerical_features + list(encoded_feature_names)\n",
    "\n",
    "# Get the coefficients from the trained linear regression model\n",
    "coefficients = pipeline.named_steps['regressor'].coef_\n",
    "\n",
    "# Create a DataFrame to view the coefficients alongside their feature names\n",
    "coef_df = pd.DataFrame({'Feature': all_feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort by the absolute value of coefficients to see which features have the most impact\n",
    "coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False).drop(columns=['Abs_Coefficient'])\n",
    "\n",
    "print(\"Feature Coefficients:\")\n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Bonus (Optional)\n",
    "\n",
    "Try using `PolynomialFeatures` to add interaction terms between some variables.\n",
    "\n",
    "Does the model improve? Or overfit?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgLUfgiFYo2_"
   },
   "outputs": [],
   "source": [
    "# PolynomialFeatures Implementation\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "print(\"=== BASELINE MODEL PERFORMANCE ===\")\n",
    "print(f\"Baseline R¬≤: {test_r2:.4f}\")\n",
    "print(f\"Baseline RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Create polynomial features using only numerical features (avoiding categorical explosion)\n",
    "numerical_only = ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'has_wifi', 'reviews_x_freq', 'days_since_last_review']\n",
    "\n",
    "print(f\"\\nNumerical features for polynomial: {numerical_only}\")\n",
    "print(f\"Shape of numerical features: {X[numerical_only].shape}\")\n",
    "\n",
    "# Create polynomial pipeline with interaction terms only\n",
    "poly_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False), numerical_only),\n",
    "        ('cat', OneHotEncoder(handle_unknown='error'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "polynomial_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', poly_preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "print(\"\\n=== TRAINING POLYNOMIAL MODEL ===\")\n",
    "polynomial_pipeline.fit(X_train, y_train)\n",
    "y_pred_poly = polynomial_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "poly_r2 = r2_score(y_test, y_pred_poly)\n",
    "poly_rmse = root_mean_squared_error(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"Polynomial R¬≤: {poly_r2:.4f}\")\n",
    "print(f\"Polynomial RMSE: {poly_rmse:.4f}\")\n",
    "\n",
    "# Cross-validation comparison\n",
    "scoring = {'RMSE': 'neg_root_mean_squared_error', 'R2': 'r2'}\n",
    "\n",
    "baseline_cv = cross_validate(pipeline, X_train, y_train, cv=5, scoring=scoring)\n",
    "poly_cv = cross_validate(polynomial_pipeline, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "baseline_cv_r2 = np.mean(baseline_cv['test_R2'])\n",
    "poly_cv_r2 = np.mean(poly_cv['test_R2'])\n",
    "\n",
    "print(\"\\n=== CROSS-VALIDATION COMPARISON ===\")\n",
    "print(f\"Baseline CV R¬≤: {baseline_cv_r2:.4f}\")\n",
    "print(f\"Polynomial CV R¬≤: {poly_cv_r2:.4f}\")\n",
    "print(f\"Improvement: {poly_cv_r2 - baseline_cv_r2:.4f}\")\n",
    "\n",
    "if poly_cv_r2 - baseline_cv_r2 > 0.05:\n",
    "    print(\"‚úÖ SIGNIFICANT IMPROVEMENT: Polynomial features help!\")\n",
    "elif poly_cv_r2 - baseline_cv_r2 > 0:\n",
    "    print(\"‚ö†Ô∏è  MARGINAL IMPROVEMENT\")  \n",
    "else:\n",
    "    print(\"‚ùå NO IMPROVEMENT: Possible overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOFrMch_Yo2_"
   },
   "source": [
    "## ‚úÖ Submission Checklist\n",
    "- [ ] Notebook is clean and readable\n",
    "- [ ] You handled weird or dirty data\n",
    "- [ ] You performed EDA\n",
    "- [ ] You trained and evaluated a regression model\n",
    "- [ ] You wrote clear comments or markdown explanations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
