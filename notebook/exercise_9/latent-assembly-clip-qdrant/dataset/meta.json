{
  "num_items": 300,
  "embedding_dim": 512,
  "model_name": "openai/clip-vit-base-patch32",
  "dataset_name": "lmms-lab/COCO-Caption2017",
  "caption_source": "sample['answer'][0]",
  "notes": "Add any observations about the dataset or extraction here"
}