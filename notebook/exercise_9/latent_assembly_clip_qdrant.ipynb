{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "569f87d0",
      "metadata": {
        "id": "569f87d0"
      },
      "source": [
        "# Latent Assembly with CLIP + Qdrant — Part A (Student Exercise)\n",
        "\n",
        "### LLM-guided debugging: build and save a reusable embedding index\n",
        "\n",
        "\n",
        "**Prerequisite:** You finished the **Qdrant indexing & search** notebook (CLIP embeddings + in-memory Qdrant).\n",
        "\n",
        "---\n",
        "\n",
        "## What you will build (Part A)\n",
        "\n",
        "A small **system-level pipeline checkpoint** that:\n",
        "\n",
        "1. Loads an image dataset from Hugging Face (you may need to try multiple options)\n",
        "2. Investigates the dataset schema and finds a usable **caption/text field**\n",
        "3. Extracts:\n",
        "   - `images` (PIL images)\n",
        "   - `payloads` (metadata with `filename` + non-empty `caption` for at least some items)\n",
        "4. Computes **CLIP image embeddings**\n",
        "5. Creates and populates an in-memory **Qdrant** collection\n",
        "6. Saves a reusable artifact package to a **private Hugging Face dataset repo**:\n",
        "   - `img_vecs.npy`\n",
        "   - `payloads.json`\n",
        "   - `meta.json` (dataset name + caption source + embedding info)\n",
        "\n",
        "You will **not** train a new model.  \n",
        "You will **not** generate images or prose with an LLM.  \n",
        "The goal is a **reusable index checkpoint** for Part B.\n",
        "\n",
        "---\n",
        "\n",
        "## Rules (important)\n",
        "\n",
        "- Some cells are **broken on purpose**. This is part of the assignment.\n",
        "- Do not “fix by guessing”. **Inspect first**:\n",
        "  - `type(x)`\n",
        "  - `x.keys()` (if available)\n",
        "  - tensor shapes (e.g., `.shape`)\n",
        "- Your payloads must include **real caption-like text** for at least some items,\n",
        "  otherwise Part B cannot do evidence aggregation.\n",
        "\n",
        "---\n",
        "\n",
        "## What to submit (Part A)\n",
        "\n",
        "- Your Hugging Face dataset repo id: `username/repo-name`\n",
        "- Confirm your repo contains:\n",
        "\n",
        "```\n",
        "latent_assembly_artifacts/\n",
        "├── img_vecs.npy\n",
        "├── payloads.json\n",
        "└── meta.json\n",
        "```\n",
        "\n",
        "- Short reflection (3–6 lines):\n",
        "  - What broke first: dataset schema, model outputs, or Qdrant API?\n",
        "  - What did you print to understand the problem before using an LLM?\n",
        "  - What is your `CAPTION_SOURCE` path?\n",
        "\n",
        "---\n",
        "\n",
        "*Part B will use this saved index to perform multi-intent retrieval and assemble a marketplace listing.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50398ba6",
      "metadata": {
        "id": "50398ba6"
      },
      "source": [
        "---\n",
        "\n",
        "# 0) Setup\n",
        "\n",
        "Run the next cells. They install/import libraries and define helper functions.\n",
        "\n",
        "Notes:\n",
        "- ~~We avoid `pip install` directly and use `ensure_package_installed(...)`.~~\n",
        "- ~~If something is already installed in your Colab runtime, it will just import it.~~\n",
        "- I am using VS Code and `uv` for development. so the `ensure_package_installed` is not needed.\n",
        "- I also moved `import` statements to the top, there were a few duplicate imports; then used `ruff` to sort the imports."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y5bT36n9ggl3",
      "metadata": {
        "id": "y5bT36n9ggl3"
      },
      "source": [
        "## How to use this notebook (quick)\n",
        "\n",
        "Some cells are **broken on purpose**.\n",
        "\n",
        "When something breaks:\n",
        "1. Read the error.\n",
        "2. Print small facts: `type(x)`, `x.keys()` (if possible), tensor shapes.\n",
        "3. Ask an LLM for help after you know what to ask.\n",
        "4. Fix, then add a tiny check (`assert` / print) to prove it works.\n",
        "\n",
        "Keep changes small and test often.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de39143f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# --- Image loader utility (safe for Colab) ---\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Imports ---\n",
        "from typing import Any\n",
        "\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from huggingface_hub import create_repo, login, upload_folder\n",
        "from PIL import Image\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as qm\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPModel, CLIPProcessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1528d2a",
      "metadata": {
        "id": "d1528d2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Utility: install + import if missing (Colab-friendly) ---\n",
        "def ensure_package_installed(package_name, import_name=None):\n",
        "    \"\"\"\n",
        "    Ensures a Python package is installed and imported.\n",
        "\n",
        "    Args:\n",
        "        package_name (str): Name used in pip install (e.g., 'torchinfo').\n",
        "        import_name (str): Module name used in import (e.g., 'torchinfo', 'sklearn').\n",
        "        Defaults to package_name.\n",
        "\n",
        "    Returns:\n",
        "        module: The imported module object.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    import_name = import_name or package_name\n",
        "\n",
        "    try:\n",
        "        return importlib.import_module(import_name)\n",
        "    except ImportError:\n",
        "        print(f\"Installing '{package_name}'...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        return importlib.import_module(import_name)\n",
        "\n",
        "# Core deps\n",
        "# np = ensure_package_installed(\"numpy\", \"numpy\")\n",
        "# tqdm_mod = ensure_package_installed(\"tqdm\", \"tqdm\")\n",
        "# torch = ensure_package_installed(\"torch\", \"torch\")\n",
        "# PIL = ensure_package_installed(\"Pillow\", \"PIL\")\n",
        "# matplotlib = ensure_package_installed(\"matplotlib\", \"matplotlib\")\n",
        "\n",
        "# datasets = ensure_package_installed(\"datasets\", \"datasets\")\n",
        "\n",
        "# Qdrant + Transformers\n",
        "# qdrant_client = ensure_package_installed(\"qdrant-client\", \"qdrant_client\")\n",
        "# transformers = ensure_package_installed(\"transformers\", \"transformers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0687f892",
      "metadata": {
        "id": "0687f892"
      },
      "outputs": [],
      "source": [
        "\n",
        "OK_RESPONSE = 200\n",
        "\n",
        "def load_image(url):\n",
        "    \"\"\"Downloads and returns a PIL image from the given URL.\"\"\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != OK_RESPONSE:\n",
        "        raise ValueError(f\"Failed to download image: {url}\")\n",
        "    try:\n",
        "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Could not open image from {url}. Error: {e}\") from e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0debf4be",
      "metadata": {
        "id": "0debf4be"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Device setup ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb2bb53",
      "metadata": {
        "id": "feb2bb53"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Load CLIP (same idea as previous notebook) ---\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval()\n",
        "\n",
        "# CLIP embedding dimension\n",
        "EMBED_DIM = model.config.projection_dim\n",
        "EMBED_DIM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c4ffc4",
      "metadata": {
        "id": "d2c4ffc4"
      },
      "source": [
        "---\n",
        "# 1) Load a small image dataset\n",
        "\n",
        "In this exercise we need **images + short text metadata** (captions), because later we will\n",
        "combine evidence across many retrieved results.\n",
        "\n",
        "To keep this notebook self-contained, the next cell loads **COCO images with captions** from Hugging Face.\n",
        "\n",
        "What you will get:\n",
        "- `images`: list of PIL images\n",
        "- `payloads`: list of dicts with:\n",
        "  - `\"filename\"`\n",
        "  - `\"caption\"`\n",
        "\n",
        "If you already have a dataset from the previous notebook and want to reuse it, you can skip the loader cell\n",
        "and replace it with your own dataset code (but make sure you still produce `images` and `payloads`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28aPC4L3j7s9",
      "metadata": {
        "id": "28aPC4L3j7s9"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_IMAGES = 300\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Dataset loading (research step)\n",
        "# ------------------------------------------------------------\n",
        "# Your goal:\n",
        "#   Load a dataset that contains:\n",
        "#     - images (or image URLs)\n",
        "#     - some human-readable text describing the image\n",
        "#\n",
        "# You are NOT guaranteed that:\n",
        "#   - the dataset loads\n",
        "#   - the split exists\n",
        "#   - the text field is obvious\n",
        "#\n",
        "# This is intentional.\n",
        "\n",
        "# Try loading ONE dataset at a time.\n",
        "# If something fails, read the error carefully.\n",
        "\n",
        "# --- YOUR CODE HERE ---\n",
        "# Examples of dataset names you *might* try:\n",
        "#   \"HuggingFaceM4/COCO\"\n",
        "#   \"lmms-lab/COCO-Caption2017\"\n",
        "#   \"detection-datasets/coco\"\n",
        "#\n",
        "# Do NOT copy-paste all of them at once.\n",
        "# Try, observe, then decide what to do next.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kcn_NyxpDJll",
      "metadata": {
        "id": "kcn_NyxpDJll"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Dataset bookkeeping (required for Part B)\n",
        "# ============================================================\n",
        "# IMPORTANT:\n",
        "# Update these TWO strings based on the dataset you actually loaded\n",
        "# and the caption path you actually extracted.\n",
        "\n",
        "DATASET_NAME = \"TODO: put the dataset name you used (e.g., lmms-lab/COCO-Caption2017)\"\n",
        "CAPTION_SOURCE = \"TODO: describe the caption path you used (e.g., sample['sentences'][0]['raw'])\"\n",
        "\n",
        "print(\"Dataset:\", DATASET_NAME)\n",
        "print(\"Caption source:\", CAPTION_SOURCE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N_JjeTOPzsS3",
      "metadata": {
        "id": "N_JjeTOPzsS3"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# Dataset investigation (do not skip)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Inspect ONE element from the dataset.\n",
        "# Do not guess the structure — print it.\n",
        "\n",
        "assert hasattr(ds, \"__iter__\"), \"Dataset is not iterable\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lj6crQ14zvHx",
      "metadata": {
        "id": "Lj6crQ14zvHx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Teaching trap:\n",
        "# This works for many \"normal\" datasets.\n",
        "# It often FAILS for a certain (very useful) type of dataset.\n",
        "# This failure is intentional in this exercise.\n",
        "\n",
        "\n",
        "\n",
        "print(\"Trying len(ds)...\")\n",
        "print(\"Dataset length:\", len(ds))   # <-- expected to crash sometimes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLpW9HWWzxZ6",
      "metadata": {
        "id": "wLpW9HWWzxZ6"
      },
      "outputs": [],
      "source": [
        "# If the previous cell crashed:\n",
        "# Your job is to explain WHY it crashed, in one sentence,\n",
        "# and then replace len(ds) with a different sanity check.\n",
        "\n",
        "# Bare hints:\n",
        "# - What is a \"streaming\" dataset conceptually?\n",
        "# - What does it mean if something is iterable but has no length?\n",
        "# - Try getting ONE sample and inspecting its keys.\n",
        "\n",
        "# Constraint:\n",
        "# - Do NOT change how the dataset is loaded.\n",
        "# - Fix the sanity check, not the dataset.\n",
        "\n",
        "raise NotImplementedError(\"TODO: write a sanity check that works for streaming datasets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o71amxnFoaEC",
      "metadata": {
        "id": "o71amxnFoaEC"
      },
      "outputs": [],
      "source": [
        "# After passing the sanity tests, check the dataset\n",
        "\n",
        "sample = next(iter(ds))\n",
        "\n",
        "print(\"Type:\", type(sample))\n",
        "print(\"Keys:\", sample.keys())\n",
        "\n",
        "# TODO:\n",
        "# 1. Pick ONE key that looks like it might contain text\n",
        "# 2. Print its type\n",
        "# 3. Print a short preview (first ~200 chars)\n",
        "\n",
        "# --- YOUR CODE HERE ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GSPnRm2cggl6",
      "metadata": {
        "id": "GSPnRm2cggl6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: Investigate -> then extract images + payloads\n",
        "# ============================================================\n",
        "# Goal:\n",
        "#   Build two lists:\n",
        "#     images   : List[PIL.Image.Image]\n",
        "#     payloads : List[dict] with keys {\"filename\", \"caption\"}\n",
        "#\n",
        "# IMPORTANT MENTAL MODEL:\n",
        "#   We do NOT care how many dataset items we *loop over*.\n",
        "#   We care how many valid items we *successfully collect*.\n",
        "#\n",
        "# That is why tqdm should count successful items,\n",
        "# not raw dataset iterations.\n",
        "#\n",
        "# ------------------------------------------------------------\n",
        "# Step 1: Investigate ONE sample (do not skip)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "sample = next(iter(ds))\n",
        "print(\"Sample keys:\", sample.keys())\n",
        "\n",
        "# TODO:\n",
        "# Pick 2–3 keys that *might* contain text.\n",
        "# For each chosen key, print:\n",
        "#   - key name\n",
        "#   - type(value)\n",
        "#   - short preview (first ~200 chars)\n",
        "#\n",
        "# Do NOT guess blindly. Inspect first.\n",
        "#\n",
        "# --- YOUR CODE HERE ---\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Step 2: Extract a dataset subset (with tqdm)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "images: list[Image.Image] = []\n",
        "payloads: list[dict[str, Any]] = []\n",
        "\n",
        "# We want tqdm to reflect how many items we ACTUALLY collected,\n",
        "# not how many dataset rows we looked at.\n",
        "pbar = tqdm(total=MAX_IMAGES, desc=\"Collecting usable samples\")\n",
        "\n",
        "# TODO:\n",
        "# Loop over ds.\n",
        "# For each sample:\n",
        "#   1) Try to extract / build a PIL image\n",
        "#      - If this fails, skip the sample (continue)\n",
        "#   2) Try to extract ONE caption string\n",
        "#      - If you truly can’t find one, use \"\" (empty string)\n",
        "#   3) Build a filename / id string\n",
        "#   4) Append to images + payloads\n",
        "#   5) ONLY THEN:\n",
        "#        - pbar.update(1)\n",
        "#   6) Stop when len(images) == MAX_IMAGES\n",
        "#\n",
        "# Bare hints:\n",
        "# - Some samples will be unusable. That is normal.\n",
        "# - tqdm.update(1) should happen ONLY on success.\n",
        "# - Do NOT assume captions are flat strings.\n",
        "\n",
        "# --- YOUR CODE HERE ---\n",
        "\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "raise NotImplementedError(\"TODO: finish extraction (images + payloads)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ofU59v6ZpoS5",
      "metadata": {
        "id": "ofU59v6ZpoS5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Sanity checks (do not delete)\n",
        "# ============================================================\n",
        "assert isinstance(images, list) and len(images) > 0, \"images is empty\"\n",
        "assert isinstance(payloads, list) and len(payloads) == len(images), \"payloads must match images length\"\n",
        "assert isinstance(payloads[0], dict), \"payloads must be a list of dicts\"\n",
        "\n",
        "has_any_caption = any(bool((p.get(\"caption\") or \"\").strip()) for p in payloads)\n",
        "\n",
        "print(\"Loaded images:\", len(images))\n",
        "print(\"Example payload:\", payloads[0])\n",
        "print(\"Has any non-empty caption:\", has_any_caption)\n",
        "\n",
        "# Strict: if captions are missing, Step 5 becomes meaningless\n",
        "assert has_any_caption, (\n",
        "    \"No captions found in payloads.\\n\"\n",
        "    \"This usually means: you extracted the wrong field, OR captions are nested.\\n\"\n",
        "    \"Go back to the investigation prints and find the correct path.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7e73094",
      "metadata": {
        "id": "c7e73094"
      },
      "source": [
        "---\n",
        "\n",
        "# 2) Compute CLIP image embeddings (scaffold provided)\n",
        "\n",
        "We will embed images in batches, normalize vectors (important for cosine similarity),\n",
        "and store embeddings as `float32` numpy arrays.\n",
        "\n",
        "You should **read** the code and make sure you understand it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MKHaSeimqNTO",
      "metadata": {
        "id": "MKHaSeimqNTO"
      },
      "source": [
        "## Reminder: model outputs may be bundles\n",
        "\n",
        "If you expected a tensor but got an object/dict:\n",
        "- print `type(x)`\n",
        "- if possible, print `x.keys()`\n",
        "- then pick the tensor field you need\n",
        "\n",
        "This is common in Transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GiUKown_x6FM",
      "metadata": {
        "id": "GiUKown_x6FM"
      },
      "outputs": [],
      "source": [
        "# Test on one image:\n",
        "\n",
        "img = images[0]\n",
        "inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
        "\n",
        "# ============================================================\n",
        "# Teaching moment: model outputs are often NOT just tensors\n",
        "# ============================================================\n",
        "# Goal: figure out what `model.get_image_features(...)` returns.\n",
        "# Rule: inspect first. Don't index into it yet.\n",
        "\n",
        "with torch.no_grad():\n",
        "    tmp = model.get_image_features(**inputs)\n",
        "\n",
        "print(\"Type of output:\", type(tmp))\n",
        "print(\"Has shape?\", hasattr(tmp, \"shape\"))\n",
        "print(\"Has keys?\", hasattr(tmp, \"keys\"))\n",
        "\n",
        "# TODO: choose ONE next step (not all):\n",
        "# - if it has shape: print(tmp.shape)\n",
        "# - if it has keys:  print(list(tmp.keys()))\n",
        "# - otherwise:       print(tmp) or dir(tmp)\n",
        "\n",
        "# --- YOUR CODE HERE ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SZ_qcEkD5BPe",
      "metadata": {
        "id": "SZ_qcEkD5BPe"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TODO: Extract the actual image embedding tensor\n",
        "# ============================================================\n",
        "# Based on your investigation:\n",
        "# - extract the tensor that represents the image embedding\n",
        "# - call it `img_embed`\n",
        "#\n",
        "# Requirements:\n",
        "# - img_embed must be a torch.Tensor\n",
        "# - shape should be (B, D)\n",
        "\n",
        "# --- YOUR CODE HERE ---\n",
        "\n",
        "assert isinstance(img_embed, torch.Tensor), \"img_embed must be a torch.Tensor\"\n",
        "assert img_embed.ndim == 2, \"Expected shape (B, D)\"\n",
        "print(\"img_embed shape:\", img_embed.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oaxOdmNRza6v",
      "metadata": {
        "id": "oaxOdmNRza6v"
      },
      "outputs": [],
      "source": [
        "def get_features_normalized(model,inputs):\n",
        "  outputs = model(**inputs)\n",
        "  feats = outputs.text_embeds        # (B, D) projected CLIP embeddings\n",
        "  feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "  return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578a1ad7",
      "metadata": {
        "id": "578a1ad7"
      },
      "outputs": [],
      "source": [
        "def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    return x / (np.linalg.norm(x, axis=-1, keepdims=True) + eps)\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_images_clip(images_pil: list[Image.Image], batch_size: int = 32) -> np.ndarray:\n",
        "    \"\"\"Returns (N, D) float32 embeddings.\"\"\"\n",
        "    all_vecs = []\n",
        "    for i in tqdm(range(0, len(images_pil), batch_size), desc=\"Embedding images\"):\n",
        "        batch = images_pil[i:i+batch_size]\n",
        "        inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        tmp = model.get_image_features(**inputs)  # TRAP: might be a tensor OR a bundle\n",
        "        # TODO: Extract the actual embedding tensor from `tmp`\n",
        "        # Bare hint: reuse what you learned in the investigation cells above.\n",
        "        # --- YOUR CODE HERE ---\n",
        "        # feats = ...\n",
        "\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)  # normalize in torch\n",
        "        all_vecs.append(feats.detach().cpu().numpy())\n",
        "\n",
        "    X = np.vstack(all_vecs).astype(np.float32)\n",
        "    X = l2_normalize(X).astype(np.float32)\n",
        "    return X\n",
        "\n",
        "# Compute image vectors\n",
        "img_vecs = embed_images_clip(images, batch_size=32)\n",
        "print(\"img_vecs:\", img_vecs.shape, img_vecs.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f86397e",
      "metadata": {
        "id": "5f86397e"
      },
      "source": [
        "---\n",
        "\n",
        "# 3) Create an in-memory Qdrant collection and upsert (review, TODO)\n",
        "\n",
        "This should feel familiar.\n",
        "\n",
        "### Requirements\n",
        "- Use in-memory Qdrant (Colab-friendly)\n",
        "- Use cosine distance\n",
        "- Store payload metadata (filename, caption, etc.)\n",
        "\n",
        "In the next cell:\n",
        "1. Create a Qdrant client\n",
        "2. Create/recreate a collection\n",
        "3. Upsert all vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf38f5e",
      "metadata": {
        "id": "0cf38f5e"
      },
      "outputs": [],
      "source": [
        "# TODO: Create Qdrant in-memory collection and then upsert vectors\n",
        "# Teaching trap: this code is intentionally incomplete.\n",
        "# Your job: inspect the client and fix API drift.\n",
        "\n",
        "COLLECTION = \"latent_assembly_images\"\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# ============================================================\n",
        "# TODO: Create Qdrant collection (intentional trap)\n",
        "# ============================================================\n",
        "# Goal:\n",
        "#   Make sure a collection named COLLECTION exists.\n",
        "#\n",
        "# Bare hint:\n",
        "#   - recreate_collection() is deprecated\n",
        "#   - newer Qdrant APIs separate:\n",
        "#       * check existence\n",
        "#       * delete (optional)\n",
        "#       * create\n",
        "#\n",
        "# Do NOT move on until the collection truly exists.\n",
        "\n",
        "# --- YOUR CODE HERE ---\n",
        "# if client.collection_exists(COLLECTION):\n",
        "#     client.delete_collection(collection_name=COLLECTION)\n",
        "#\n",
        "# client.create_collection(\n",
        "#     collection_name=COLLECTION,\n",
        "#     vectors_config=qm.VectorParams(size=EMBED_DIM, distance=qm.Distance.COSINE),\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3xP48nIrBe-O",
      "metadata": {
        "id": "3xP48nIrBe-O"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Guardrail: verify collection exists BEFORE upsert\n",
        "# ============================================================\n",
        "# This cell is here so you don't blindly run upsert() and get a confusing error.\n",
        "\n",
        "print(\"Collection name:\", COLLECTION)\n",
        "\n",
        "exists = client.collection_exists(COLLECTION)\n",
        "print(\"collection_exists:\", exists)\n",
        "\n",
        "assert exists, (\n",
        "    f\"Collection '{COLLECTION}' does not exist yet.\\n\"\n",
        "    \"Fix the collection creation step BEFORE running upsert.\\n\"\n",
        "    \"Bare hint: newer Qdrant uses create_collection (and optionally delete_collection).\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vgcoKds0-Wv1",
      "metadata": {
        "id": "vgcoKds0-Wv1"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Upsert points into Qdrant\n",
        "# ============================================================\n",
        "\n",
        "ids = list(range(len(img_vecs)))\n",
        "\n",
        "points = [\n",
        "    qm.PointStruct(\n",
        "        id=ids[i],\n",
        "        vector=img_vecs[i].tolist(),\n",
        "        payload=payloads[i],\n",
        "    )\n",
        "    for i in range(len(ids))\n",
        "]\n",
        "\n",
        "client.upsert(collection_name=COLLECTION, points=points)\n",
        "\n",
        "print(\"Collection populated.\")\n",
        "print(\"Count:\", client.count(collection_name=COLLECTION, exact=True).count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt61KFFCwlO_",
      "metadata": {
        "id": "Nt61KFFCwlO_"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Sanity check: can we retrieve anything? (1 query only)\n",
        "# ============================================================\n",
        "# Goal:\n",
        "# - run ONE text query\n",
        "# - get TOP_K hits\n",
        "# - print payload keys + a caption preview from the first hit\n",
        "#\n",
        "# This is NOT the full multi-intent stage (that’s Part B).\n",
        "# This is just a basic “does retrieval work at all?” check.\n",
        "\n",
        "TOP_K = 3\n",
        "test_query = \"chair\"   # keep it boring on purpose\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_text_clip(text: str) -> np.ndarray:\n",
        "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    # Note: this mirrors the image side — text features may also come back as a bundle.\n",
        "    feats = model.get_text_features(**inputs)\n",
        "\n",
        "    # TRAP: feats may be a bundle (dict/object). Extract the tensor.\n",
        "    # --- YOUR CODE HERE ---\n",
        "    # Example (do not assume): feats = feats[\"pooler_output\"]\n",
        "\n",
        "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    v = feats.detach().cpu().numpy().astype(np.float32)[0]\n",
        "    return l2_normalize(v[None, :])[0].astype(np.float32)\n",
        "\n",
        "qv = embed_text_clip(test_query)\n",
        "\n",
        "# TRAP: Qdrant API may differ by version.\n",
        "# Your job: make ONE of these work.\n",
        "# Hint: inspect dir(client)\n",
        "hits = None\n",
        "\n",
        "# Option A (some versions)\n",
        "if hasattr(client, \"search\"):\n",
        "    hits = client.search(\n",
        "        collection_name=COLLECTION,\n",
        "        query_vector=qv.tolist(),\n",
        "        limit=TOP_K,\n",
        "        with_payload=True,\n",
        "    )\n",
        "else:\n",
        "    # Option B (newer versions)\n",
        "    # TODO: find the correct method name + arguments in your Qdrant version\n",
        "    # --- YOUR CODE HERE ---\n",
        "    raise NotImplementedError(\"TODO: implement search for your client version\")\n",
        "\n",
        "assert hits is not None and len(hits) > 0, \"No hits returned. Retrieval not working.\"\n",
        "\n",
        "print(\"Query:\", test_query)\n",
        "print(\"Hits:\", len(hits))\n",
        "\n",
        "h0 = hits[0]\n",
        "payload0 = getattr(h0, \"payload\", None) or {}\n",
        "print(\"Payload keys:\", list(payload0.keys()))\n",
        "\n",
        "caption_preview = (payload0.get(\"caption\") or payload0.get(\"filename\") or \"\")\n",
        "print(\"Preview:\", str(caption_preview)[:140])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n80PqWjXggl7",
      "metadata": {
        "id": "n80PqWjXggl7"
      },
      "source": [
        "# Save to Hugging Face (for Notebook B)\n",
        "\n",
        "By the end of Part A, your Hugging Face dataset repo must contain **exactly these files**:\n",
        "\n",
        "```\n",
        "latent_assembly_artifacts/\n",
        "├── img_vecs.npy      # image embeddings (shape: N × D)\n",
        "├── payloads.json     # metadata per image (filename + caption)\n",
        "└── meta.json         # index metadata (dataset, model, caption source)\n",
        "```\n",
        "\n",
        "Use a **private** Hugging Face *dataset* repo under your own account.\n",
        "\n",
        "Notebook B will download this folder and rebuild Qdrant **without re-embedding**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7wFNkBq9DVQs",
      "metadata": {
        "id": "7wFNkBq9DVQs"
      },
      "source": [
        "## Metadata for reuse (important)\n",
        "\n",
        "Before uploading your embeddings, you must save a small `meta.json` file.\n",
        "\n",
        "This file explains:\n",
        "- what dataset you used\n",
        "- how many items are indexed\n",
        "- what model and embedding size were used\n",
        "- where captions came from in the dataset schema\n",
        "\n",
        "This is required so **Part B can rebuild Qdrant without re-embedding**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BX-ZzLVHggl7",
      "metadata": {
        "id": "BX-ZzLVHggl7"
      },
      "source": [
        "## Colab Secrets token\n",
        "\n",
        "Colab left sidebar → key icon (Secrets) → add:\n",
        "- Name: HF_TOKEN\n",
        "- Value: your HF token (Write permission)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xmBp7PNWDbxC",
      "metadata": {
        "id": "xmBp7PNWDbxC"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Save metadata about this embedding index\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "ARTIFACT_DIR = Path(\"latent_assembly_artifacts\")\n",
        "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# TODO (small but important):\n",
        "# You should already have DATASET_NAME and CAPTION_SOURCE defined earlier.\n",
        "# This metadata will be used by Part B to rebuild Qdrant without guessing.\n",
        "\n",
        "meta = {\n",
        "    \"num_items\": int(len(payloads)),\n",
        "    \"embedding_dim\": int(img_vecs.shape[1]),\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"dataset_name\": DATASET_NAME,\n",
        "    \"caption_source\": CAPTION_SOURCE,\n",
        "    \"notes\": \"Add any observations about the dataset or extraction here\"\n",
        "}\n",
        "\n",
        "with open(ARTIFACT_DIR / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved meta.json:\")\n",
        "print(json.dumps(meta, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KfaZfmEWggl7",
      "metadata": {
        "id": "KfaZfmEWggl7"
      },
      "outputs": [],
      "source": [
        "# TODO: Upload artifacts to your private HF dataset repo\n",
        "\n",
        "# huggingface_hub = ensure_package_installed(\"huggingface_hub\", \"huggingface_hub\")\n",
        "\n",
        "\n",
        "try:\n",
        "    # from google.colab import userdata\n",
        "    # HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "except Exception:\n",
        "    HF_TOKEN = None\n",
        "\n",
        "assert HF_TOKEN, \"HF_TOKEN not found. Add it in Colab Secrets (key icon).\"\n",
        "login(token=HF_TOKEN)\n",
        "\n",
        "# TODO: change to your own repo id, under your username\n",
        "HF_REPO_ID = \"YOUR_USERNAME/YOUR_DATASET_REPO_NAME\"\n",
        "\n",
        "ARTIFACT_DIR = Path(\"latent_assembly_artifacts\")\n",
        "ARTIFACT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "np.save(ARTIFACT_DIR / \"img_vecs.npy\", img_vecs)\n",
        "with open(ARTIFACT_DIR / \"payloads.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(payloads, f, ensure_ascii=False)\n",
        "\n",
        "# IMPORTANT: do NOT overwrite meta.json here.\n",
        "# It must include dataset_name + caption_source from the metadata cell above.\n",
        "assert (ARTIFACT_DIR / \"meta.json\").exists(), (\n",
        "    \"meta.json is missing. Run the metadata cell above before uploading.\"\n",
        ")\n",
        "\n",
        "create_repo(repo_id=HF_REPO_ID, repo_type=\"dataset\", private=True, exist_ok=True)\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=HF_REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    folder_path=str(ARTIFACT_DIR),\n",
        "    path_in_repo=\"artifacts\",\n",
        "    commit_message=\"Upload Latent Assembly artifacts\"\n",
        ")\n",
        "\n",
        "print(\"Uploaded:\", HF_REPO_ID)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xciLH-Oeggl7",
      "metadata": {
        "id": "xciLH-Oeggl7"
      },
      "source": [
        "## Submit (Part A)\n",
        "\n",
        "Submit your HF repo id: `username/repo-name`\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data-science",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
