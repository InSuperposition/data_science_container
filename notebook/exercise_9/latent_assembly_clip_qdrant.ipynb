{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569f87d0",
   "metadata": {
    "id": "569f87d0"
   },
   "source": [
    "# Latent Assembly with CLIP + Qdrant — Part A (Student Exercise)\n",
    "\n",
    "### LLM-guided debugging: build and save a reusable embedding index\n",
    "\n",
    "\n",
    "**Prerequisite:** You finished the **Qdrant indexing & search** notebook (CLIP embeddings + in-memory Qdrant).\n",
    "\n",
    "---\n",
    "\n",
    "## What you will build (Part A)\n",
    "\n",
    "A small **system-level pipeline checkpoint** that:\n",
    "\n",
    "1. Loads an image dataset from Hugging Face (you may need to try multiple options)\n",
    "2. Investigates the dataset schema and finds a usable **caption/text field**\n",
    "3. Extracts:\n",
    "   - `images` (PIL images)\n",
    "   - `payloads` (metadata with `filename` + non-empty `caption` for at least some items)\n",
    "4. Computes **CLIP image embeddings**\n",
    "5. Creates and populates an in-memory **Qdrant** collection\n",
    "6. Saves a reusable artifact package to a **private Hugging Face dataset repo**:\n",
    "   - `img_vecs.npy`\n",
    "   - `payloads.json`\n",
    "   - `meta.json` (dataset name + caption source + embedding info)\n",
    "\n",
    "You will **not** train a new model.  \n",
    "You will **not** generate images or prose with an LLM.  \n",
    "The goal is a **reusable index checkpoint** for Part B.\n",
    "\n",
    "---\n",
    "\n",
    "## Rules (important)\n",
    "\n",
    "- Some cells are **broken on purpose**. This is part of the assignment.\n",
    "- Do not “fix by guessing”. **Inspect first**:\n",
    "  - `type(x)`\n",
    "  - `x.keys()` (if available)\n",
    "  - tensor shapes (e.g., `.shape`)\n",
    "- Your payloads must include **real caption-like text** for at least some items,\n",
    "  otherwise Part B cannot do evidence aggregation.\n",
    "\n",
    "---\n",
    "\n",
    "## What to submit (Part A)\n",
    "\n",
    "- Your Hugging Face dataset repo id: `username/repo-name`\n",
    "- Confirm your repo contains:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── img_vecs.npy\n",
    "├── payloads.json\n",
    "└── meta.json\n",
    "```\n",
    "\n",
    "- Short reflection (3–6 lines):\n",
    "  - What broke first: dataset schema, model outputs, or Qdrant API?\n",
    "  - What did you print to understand the problem before using an LLM?\n",
    "  - What is your `CAPTION_SOURCE` path?\n",
    "\n",
    "---\n",
    "\n",
    "*Part B will use this saved index to perform multi-intent retrieval and assemble a marketplace listing.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50398ba6",
   "metadata": {
    "id": "50398ba6"
   },
   "source": [
    "---\n",
    "\n",
    "# 0) Setup\n",
    "\n",
    "Run the next cells. They install/import libraries and define helper functions.\n",
    "\n",
    "Notes:\n",
    "- ~~We avoid `pip install` directly and use `ensure_package_installed(...)`.~~\n",
    "- ~~If something is already installed in your Colab runtime, it will just import it.~~\n",
    "- I am using VS Code and `uv` for development. so the `ensure_package_installed` is not needed.\n",
    "- I also moved `import` statements to the top, there were a few duplicate imports; then used `ruff` to sort the imports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y5bT36n9ggl3",
   "metadata": {
    "id": "y5bT36n9ggl3"
   },
   "source": [
    "## How to use this notebook (quick)\n",
    "\n",
    "Some cells are **broken on purpose**.\n",
    "\n",
    "When something breaks:\n",
    "1. Read the error.\n",
    "2. Print small facts: `type(x)`, `x.keys()` (if possible), tensor shapes.\n",
    "3. Ask an LLM for help after you know what to ask.\n",
    "4. Fix, then add a tiny check (`assert` / print) to prove it works.\n",
    "\n",
    "Keep changes small and test often.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de39143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# --- Image loader utility (safe for Colab) ---\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Imports ---\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, login, upload_folder\n",
    "from PIL import Image\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qm\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1528d2a",
   "metadata": {
    "id": "d1528d2a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Utility: install + import if missing (Colab-friendly) ---\n",
    "def ensure_package_installed(package_name, import_name=None):\n",
    "    \"\"\"\n",
    "    Ensures a Python package is installed and imported.\n",
    "\n",
    "    Args:\n",
    "        package_name (str): Name used in pip install (e.g., 'torchinfo').\n",
    "        import_name (str): Module name used in import (e.g., 'torchinfo', 'sklearn').\n",
    "        Defaults to package_name.\n",
    "\n",
    "    Returns:\n",
    "        module: The imported module object.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    import_name = import_name or package_name\n",
    "\n",
    "    try:\n",
    "        return importlib.import_module(import_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing '{package_name}'...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        return importlib.import_module(import_name)\n",
    "\n",
    "# Core deps\n",
    "# np = ensure_package_installed(\"numpy\", \"numpy\")\n",
    "# tqdm_mod = ensure_package_installed(\"tqdm\", \"tqdm\")\n",
    "# torch = ensure_package_installed(\"torch\", \"torch\")\n",
    "# PIL = ensure_package_installed(\"Pillow\", \"PIL\")\n",
    "# matplotlib = ensure_package_installed(\"matplotlib\", \"matplotlib\")\n",
    "\n",
    "# datasets = ensure_package_installed(\"datasets\", \"datasets\")\n",
    "\n",
    "# Qdrant + Transformers\n",
    "# qdrant_client = ensure_package_installed(\"qdrant-client\", \"qdrant_client\")\n",
    "# transformers = ensure_package_installed(\"transformers\", \"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0687f892",
   "metadata": {
    "id": "0687f892"
   },
   "outputs": [],
   "source": [
    "\n",
    "OK_RESPONSE = 200\n",
    "\n",
    "def load_image(url):\n",
    "    \"\"\"Downloads and returns a PIL image from the given URL.\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != OK_RESPONSE:\n",
    "        raise ValueError(f\"Failed to download image: {url}\")\n",
    "    try:\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not open image from {url}. Error: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0debf4be",
   "metadata": {
    "id": "0debf4be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Device setup ---\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb2bb53",
   "metadata": {
    "id": "feb2bb53"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecff245ed8b49dab5525b8b2a5d335a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load CLIP (same idea as previous notebook) ---\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# CLIP embedding dimension\n",
    "EMBED_DIM = model.config.projection_dim\n",
    "EMBED_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c4ffc4",
   "metadata": {
    "id": "d2c4ffc4"
   },
   "source": [
    "---\n",
    "# 1) Load a small image dataset\n",
    "\n",
    "In this exercise we need **images + short text metadata** (captions), because later we will\n",
    "combine evidence across many retrieved results.\n",
    "\n",
    "To keep this notebook self-contained, the next cell loads **COCO images with captions** from Hugging Face.\n",
    "\n",
    "What you will get:\n",
    "- `images`: list of PIL images\n",
    "- `payloads`: list of dicts with:\n",
    "  - `\"filename\"`\n",
    "  - `\"caption\"`\n",
    "\n",
    "If you already have a dataset from the previous notebook and want to reuse it, you can skip the loader cell\n",
    "and replace it with your own dataset code (but make sure you still produce `images` and `payloads`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28aPC4L3j7s9",
   "metadata": {
    "id": "28aPC4L3j7s9"
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_IMAGES = 300\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dataset loading (research step)\n",
    "# ------------------------------------------------------------\n",
    "# Your goal:\n",
    "#   Load a dataset that contains:\n",
    "#     - images (or image URLs)\n",
    "#     - some human-readable text describing the image\n",
    "#\n",
    "# You are NOT guaranteed that:\n",
    "#   - the dataset loads\n",
    "#   - the split exists\n",
    "#   - the text field is obvious\n",
    "#\n",
    "# This is intentional.\n",
    "\n",
    "# Try loading ONE dataset at a time.\n",
    "# If something fails, read the error carefully.\n",
    "\n",
    "# --- YOUR CODE HERE ---\n",
    "# Examples of dataset names you *might* try:\n",
    "#   \"HuggingFaceM4/COCO\"\n",
    "#   \"lmms-lab/COCO-Caption2017\"\n",
    "#   \"detection-datasets/coco\"\n",
    "#\n",
    "# Do NOT copy-paste all of them at once.\n",
    "# Try, observe, then decide what to do next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "kcn_NyxpDJll",
   "metadata": {
    "id": "kcn_NyxpDJll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: lmms-lab/COCO-Caption2017\n",
      "Caption source: sample['answer'][0]\n",
      "Dataset type: <class 'datasets.iterable_dataset.IterableDataset'>\n",
      "Streaming dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Dataset bookkeeping (required for Part B)\n",
    "# ============================================================\n",
    "# IMPORTANT:\n",
    "# Update these TWO strings based on the dataset you actually loaded\n",
    "# and the caption path you actually extracted.\n",
    "#\n",
    "# Original TODO placeholders:\n",
    "# DATASET_NAME = \"TODO: put the dataset name you used (e.g., lmms-lab/COCO-Caption2017)\"\n",
    "# CAPTION_SOURCE = \"TODO: describe the caption path you used (e.g., sample['sentences'][0]['raw'])\"\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "DATASET_NAME = \"lmms-lab/COCO-Caption2017\"\n",
    "CAPTION_SOURCE = \"sample['answer'][0]\"\n",
    "\n",
    "print(\"Dataset:\", DATASET_NAME)\n",
    "print(\"Caption source:\", CAPTION_SOURCE)\n",
    "\n",
    "ds = load_dataset(DATASET_NAME, split=\"val\", streaming=True)\n",
    "print(\"Dataset type:\", type(ds))\n",
    "print(\"Streaming dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "N_JjeTOPzsS3",
   "metadata": {
    "id": "N_JjeTOPzsS3"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Dataset investigation (do not skip)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Inspect ONE element from the dataset.\n",
    "# Do not guess the structure — print it.\n",
    "\n",
    "assert hasattr(ds, \"__iter__\"), \"Dataset is not iterable\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Lj6crQ14zvHx",
   "metadata": {
    "id": "Lj6crQ14zvHx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped len(ds) — streaming datasets have no __len__.\n"
     ]
    }
   ],
   "source": [
    "# Teaching trap:\n",
    "# This works for many \"normal\" datasets.\n",
    "# It often FAILS for a certain (very useful) type of dataset.\n",
    "# This failure is intentional in this exercise.\n",
    "\n",
    "# EXPECTED CRASH — uncomment to demonstrate:\n",
    "# print(\"Trying len(ds)...\")\n",
    "# print(\"Dataset length:\", len(ds))   # TypeError: object of type 'IterableDataset' has no len()\n",
    "\n",
    "# LESSON: Streaming datasets are generators. They yield one sample at a time\n",
    "# over the network. There is no way to know the total count without consuming\n",
    "# the entire stream. Use next(iter(ds)) to peek at the structure instead.\n",
    "print(\"Skipped len(ds) — streaming datasets have no __len__.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wLpW9HWWzxZ6",
   "metadata": {
    "id": "wLpW9HWWzxZ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check passed: got one sample\n",
      "Sample keys: ['question_id', 'image', 'question', 'answer', 'id', 'license', 'file_name', 'coco_url', 'height', 'width', 'date_captured']\n"
     ]
    }
   ],
   "source": [
    "# If the previous cell crashed:\n",
    "# Your job is to explain WHY it crashed, in one sentence,\n",
    "# and then replace len(ds) with a different sanity check.\n",
    "\n",
    "# Bare hints:\n",
    "# - What is a \"streaming\" dataset conceptually?\n",
    "# - What does it mean if something is iterable but has no length?\n",
    "# - Try getting ONE sample and inspecting its keys.\n",
    "\n",
    "# Constraint:\n",
    "# - Do NOT change how the dataset is loaded.\n",
    "# - Fix the sanity check, not the dataset.\n",
    "\n",
    "# Original TODO:\n",
    "# raise NotImplementedError(\"TODO: write a sanity check that works for streaming datasets\")\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "# Streaming datasets have no len(). Instead, peek at one sample\n",
    "# to verify the dataset is iterable and non-empty.\n",
    "\n",
    "sample = next(iter(ds))\n",
    "assert sample is not None, \"Dataset yielded None — something is wrong\"\n",
    "print(\"Sanity check passed: got one sample\")\n",
    "print(\"Sample keys:\", list(sample.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "o71amxnFoaEC",
   "metadata": {
    "id": "o71amxnFoaEC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Keys: ['question_id', 'image', 'question', 'answer', 'id', 'license', 'file_name', 'coco_url', 'height', 'width', 'date_captured']\n",
      "\n",
      "  question_id: type=str, preview=000000179765.jpg\n",
      "\n",
      "  image: type=JpegImageFile, preview=<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x13ABFC190>\n",
      "\n",
      "  question: type=str, preview=Please carefully observe the image and come up with a caption for the image.\n",
      "\n",
      "  answer: type=list, preview=['A black Honda motorcycle parked in front of a garage.', 'A Honda motorcycle parked in a grass driveway', 'A black Honda motorcycle with a dark burgundy seat.', 'Ma motorcycle parked on the gravel in\n",
      "\n",
      "  id: type=int, preview=38\n",
      "\n",
      "  license: type=int, preview=3\n",
      "\n",
      "  file_name: type=str, preview=000000179765.jpg\n",
      "\n",
      "  coco_url: type=str, preview=http://images.cocodataset.org/val2017/000000179765.jpg\n",
      "\n",
      "  height: type=int, preview=480\n",
      "\n",
      "  width: type=int, preview=640\n",
      "\n",
      "  date_captured: type=str, preview=2013-11-15 14:02:51\n",
      "\n",
      "Caption field path: sample['answer'][0]\n",
      "Example caption: A black Honda motorcycle parked in front of a garage.\n"
     ]
    }
   ],
   "source": [
    "# After passing the sanity tests, check the dataset\n",
    "\n",
    "sample = next(iter(ds))\n",
    "\n",
    "print(\"Type:\", type(sample))\n",
    "print(\"Keys:\", list(sample.keys()))\n",
    "\n",
    "# TODO:\n",
    "# 1. Pick ONE key that looks like it might contain text\n",
    "# 2. Print its type\n",
    "# 3. Print a short preview (first ~200 chars)\n",
    "\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "print()\n",
    "\n",
    "# Investigate text-like fields\n",
    "for key in sample.keys():\n",
    "    val = sample[key]\n",
    "    print(f\"  {key}: type={type(val).__name__}, preview={str(val)[:200]}\")\n",
    "    print()\n",
    "\n",
    "# The caption field is \"answer\" — it's a LIST of strings (one per annotator).\n",
    "# We'll use answer[0] as our caption.\n",
    "print(\"Caption field path: sample['answer'][0]\")\n",
    "print(\"Example caption:\", sample[\"answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "GSPnRm2cggl6",
   "metadata": {
    "id": "GSPnRm2cggl6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting usable samples: 100%|██████████| 300/300 [00:01<00:00, 188.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 300 images with 300 captions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TODO: Investigate -> then extract images + payloads\n",
    "# ============================================================\n",
    "# Goal:\n",
    "#   Build two lists:\n",
    "#     images   : List[PIL.Image.Image]\n",
    "#     payloads : List[dict] with keys {\"filename\", \"caption\"}\n",
    "#\n",
    "# IMPORTANT MENTAL MODEL:\n",
    "#   We do NOT care how many dataset items we *loop over*.\n",
    "#   We care how many valid items we *successfully collect*.\n",
    "#\n",
    "# That is why tqdm should count successful items,\n",
    "# not raw dataset iterations.\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Investigate ONE sample (do not skip)\n",
    "# ------------------------------------------------------------\n",
    "# TODO:\n",
    "# Pick 2–3 keys that *might* contain text.\n",
    "# For each chosen key, print:\n",
    "#   - key name\n",
    "#   - type(value)\n",
    "#   - short preview (first ~200 chars)\n",
    "#\n",
    "# Do NOT guess blindly. Inspect first.\n",
    "#\n",
    "# --- YOUR CODE HERE ---\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Extract a dataset subset (with tqdm)\n",
    "# ------------------------------------------------------------\n",
    "# TODO:\n",
    "# Loop over ds.\n",
    "# For each sample:\n",
    "#   1) Try to extract / build a PIL image\n",
    "#      - If this fails, skip the sample (continue)\n",
    "#   2) Try to extract ONE caption string\n",
    "#      - If you truly can't find one, use \"\" (empty string)\n",
    "#   3) Build a filename / id string\n",
    "#   4) Append to images + payloads\n",
    "#   5) ONLY THEN:\n",
    "#        - pbar.update(1)\n",
    "#   6) Stop when len(images) == MAX_IMAGES\n",
    "#\n",
    "# Bare hints:\n",
    "# - Some samples will be unusable. That is normal.\n",
    "# - tqdm.update(1) should happen ONLY on success.\n",
    "# - Do NOT assume captions are flat strings.\n",
    "#\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "images: list[Image.Image] = []\n",
    "payloads: list[dict[str, Any]] = []\n",
    "\n",
    "pbar = tqdm(total=MAX_IMAGES, desc=\"Collecting usable samples\")\n",
    "\n",
    "for sample in ds:\n",
    "    try:\n",
    "        # Extract image — the dataset provides PIL images directly\n",
    "        img = sample[\"image\"]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            continue\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        # Extract caption from nested list (field is \"answer\", not \"sentences_raw\")\n",
    "        captions = sample.get(\"answer\", [])\n",
    "        caption = captions[0] if captions else \"\"\n",
    "\n",
    "        # Build filename from the sample\n",
    "        filename = sample.get(\"file_name\", f\"img_{len(images):05d}.jpg\")\n",
    "\n",
    "        images.append(img)\n",
    "        payloads.append({\"filename\": filename, \"caption\": caption})\n",
    "        pbar.update(1)\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    if len(images) >= MAX_IMAGES:\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(f\"Collected {len(images)} images with {sum(1 for p in payloads if p['caption'])} captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ofU59v6ZpoS5",
   "metadata": {
    "id": "ofU59v6ZpoS5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images: 300\n",
      "Example payload: {'filename': '000000179765.jpg', 'caption': 'A black Honda motorcycle parked in front of a garage.'}\n",
      "Has any non-empty caption: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Sanity checks (do not delete)\n",
    "# ============================================================\n",
    "assert isinstance(images, list) and len(images) > 0, \"images is empty\"\n",
    "assert isinstance(payloads, list) and len(payloads) == len(images), \"payloads must match images length\"\n",
    "assert isinstance(payloads[0], dict), \"payloads must be a list of dicts\"\n",
    "\n",
    "has_any_caption = any(bool((p.get(\"caption\") or \"\").strip()) for p in payloads)\n",
    "\n",
    "print(\"Loaded images:\", len(images))\n",
    "print(\"Example payload:\", payloads[0])\n",
    "print(\"Has any non-empty caption:\", has_any_caption)\n",
    "\n",
    "# Strict: if captions are missing, Step 5 becomes meaningless\n",
    "assert has_any_caption, (\n",
    "    \"No captions found in payloads.\\n\"\n",
    "    \"This usually means: you extracted the wrong field, OR captions are nested.\\n\"\n",
    "    \"Go back to the investigation prints and find the correct path.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e73094",
   "metadata": {
    "id": "c7e73094"
   },
   "source": [
    "---\n",
    "\n",
    "# 2) Compute CLIP image embeddings (scaffold provided)\n",
    "\n",
    "We will embed images in batches, normalize vectors (important for cosine similarity),\n",
    "and store embeddings as `float32` numpy arrays.\n",
    "\n",
    "You should **read** the code and make sure you understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MKHaSeimqNTO",
   "metadata": {
    "id": "MKHaSeimqNTO"
   },
   "source": [
    "## Reminder: model outputs may be bundles\n",
    "\n",
    "If you expected a tensor but got an object/dict:\n",
    "- print `type(x)`\n",
    "- if possible, print `x.keys()`\n",
    "- then pick the tensor field you need\n",
    "\n",
    "This is common in Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "GiUKown_x6FM",
   "metadata": {
    "id": "GiUKown_x6FM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of output: <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
      "Has shape? False\n",
      "Has keys? True\n",
      "Keys: ['last_hidden_state', 'pooler_output']\n",
      "pooler_output shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# Test on one image:\n",
    "\n",
    "img = images[0]\n",
    "inputs = processor(images=[img], return_tensors=\"pt\").to(device)\n",
    "\n",
    "# ============================================================\n",
    "# Teaching moment: model outputs are often NOT just tensors\n",
    "# ============================================================\n",
    "# Goal: figure out what `model.get_image_features(...)` returns.\n",
    "# Rule: inspect first. Don't index into it yet.\n",
    "\n",
    "with torch.no_grad():\n",
    "    tmp = model.get_image_features(**inputs)\n",
    "\n",
    "print(\"Type of output:\", type(tmp))\n",
    "print(\"Has shape?\", hasattr(tmp, \"shape\"))\n",
    "print(\"Has keys?\", hasattr(tmp, \"keys\"))\n",
    "\n",
    "# TODO: choose ONE next step (not all):\n",
    "# - if it has shape: print(tmp.shape)\n",
    "# - if it has keys:  print(list(tmp.keys()))\n",
    "# - otherwise:       print(tmp) or dir(tmp)\n",
    "\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "# It's a bundle! Inspect the keys.\n",
    "print(\"Keys:\", list(tmp.keys()))\n",
    "print(\"pooler_output shape:\", tmp.pooler_output.shape)  # Expected: (1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "SZ_qcEkD5BPe",
   "metadata": {
    "id": "SZ_qcEkD5BPe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_embed shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TODO: Extract the actual image embedding tensor\n",
    "# ============================================================\n",
    "# Based on your investigation:\n",
    "# - extract the tensor that represents the image embedding\n",
    "# - call it `img_embed`\n",
    "#\n",
    "# Requirements:\n",
    "# - img_embed must be a torch.Tensor\n",
    "# - shape should be (B, D)\n",
    "\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "# get_image_features() returns a bundle with pooler_output as the\n",
    "# projected (B, 512) embedding tensor.\n",
    "\n",
    "img_embed = tmp.pooler_output\n",
    "\n",
    "assert isinstance(img_embed, torch.Tensor), \"img_embed must be a torch.Tensor\"\n",
    "assert img_embed.ndim == 2, \"Expected shape (B, D)\"\n",
    "print(\"img_embed shape:\", img_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "oaxOdmNRza6v",
   "metadata": {
    "id": "oaxOdmNRza6v"
   },
   "outputs": [],
   "source": [
    "def get_features_normalized(model,inputs):\n",
    "  outputs = model(**inputs)\n",
    "  feats = outputs.text_embeds        # (B, D) projected CLIP embeddings\n",
    "  feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "  return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "578a1ad7",
   "metadata": {
    "id": "578a1ad7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding images: 100%|██████████| 10/10 [00:01<00:00,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_vecs: (300, 512) float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    return x / (np.linalg.norm(x, axis=-1, keepdims=True) + eps)\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_images_clip(images_pil: list[Image.Image], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"Returns (N, D) float32 embeddings.\"\"\"\n",
    "    all_vecs = []\n",
    "    for i in tqdm(range(0, len(images_pil), batch_size), desc=\"Embedding images\"):\n",
    "        batch = images_pil[i:i+batch_size]\n",
    "        inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        tmp = model.get_image_features(**inputs)  # TRAP: might be a tensor OR a bundle\n",
    "        # TODO: Extract the actual embedding tensor from `tmp`\n",
    "        # Bare hint: reuse what you learned in the investigation cells above.\n",
    "        # --- YOUR CODE HERE ---\n",
    "        # feats = ...\n",
    "\n",
    "        # SOLUTION (filled in):\n",
    "        feats = tmp.pooler_output\n",
    "\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)  # normalize in torch\n",
    "        all_vecs.append(feats.detach().cpu().numpy())\n",
    "\n",
    "    X = np.vstack(all_vecs).astype(np.float32)\n",
    "    X = l2_normalize(X).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "# Compute image vectors\n",
    "img_vecs = embed_images_clip(images, batch_size=32)\n",
    "print(\"img_vecs:\", img_vecs.shape, img_vecs.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86397e",
   "metadata": {
    "id": "5f86397e"
   },
   "source": [
    "---\n",
    "\n",
    "# 3) Create an in-memory Qdrant collection and upsert (review, TODO)\n",
    "\n",
    "This should feel familiar.\n",
    "\n",
    "### Requirements\n",
    "- Use in-memory Qdrant (Colab-friendly)\n",
    "- Use cosine distance\n",
    "- Store payload metadata (filename, caption, etc.)\n",
    "\n",
    "In the next cell:\n",
    "1. Create a Qdrant client\n",
    "2. Create/recreate a collection\n",
    "3. Upsert all vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cf38f5e",
   "metadata": {
    "id": "0cf38f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'latent_assembly_images' created with dim=512, distance=COSINE\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create Qdrant in-memory collection and then upsert vectors\n",
    "# Teaching trap: this code is intentionally incomplete.\n",
    "# Your job: inspect the client and fix API drift.\n",
    "\n",
    "COLLECTION = \"latent_assembly_images\"\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# ============================================================\n",
    "# TODO: Create Qdrant collection (intentional trap)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "#   Make sure a collection named COLLECTION exists.\n",
    "#\n",
    "# Bare hint:\n",
    "#   - recreate_collection() is deprecated\n",
    "#   - newer Qdrant APIs separate:\n",
    "#       * check existence\n",
    "#       * delete (optional)\n",
    "#       * create\n",
    "#\n",
    "# Do NOT move on until the collection truly exists.\n",
    "\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "# Modern Qdrant API: explicit delete + create (not deprecated recreate_collection)\n",
    "if client.collection_exists(COLLECTION):\n",
    "    client.delete_collection(collection_name=COLLECTION)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION,\n",
    "    vectors_config=qm.VectorParams(size=EMBED_DIM, distance=qm.Distance.COSINE),\n",
    ")\n",
    "\n",
    "print(f\"Collection '{COLLECTION}' created with dim={EMBED_DIM}, distance=COSINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3xP48nIrBe-O",
   "metadata": {
    "id": "3xP48nIrBe-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name: latent_assembly_images\n",
      "collection_exists: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Guardrail: verify collection exists BEFORE upsert\n",
    "# ============================================================\n",
    "# This cell is here so you don't blindly run upsert() and get a confusing error.\n",
    "\n",
    "print(\"Collection name:\", COLLECTION)\n",
    "\n",
    "exists = client.collection_exists(COLLECTION)\n",
    "print(\"collection_exists:\", exists)\n",
    "\n",
    "assert exists, (\n",
    "    f\"Collection '{COLLECTION}' does not exist yet.\\n\"\n",
    "    \"Fix the collection creation step BEFORE running upsert.\\n\"\n",
    "    \"Bare hint: newer Qdrant uses create_collection (and optionally delete_collection).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vgcoKds0-Wv1",
   "metadata": {
    "id": "vgcoKds0-Wv1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection populated.\n",
      "Count: 300\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Upsert points into Qdrant\n",
    "# ============================================================\n",
    "\n",
    "ids = list(range(len(img_vecs)))\n",
    "\n",
    "points = [\n",
    "    qm.PointStruct(\n",
    "        id=ids[i],\n",
    "        vector=img_vecs[i].tolist(),\n",
    "        payload=payloads[i],\n",
    "    )\n",
    "    for i in range(len(ids))\n",
    "]\n",
    "\n",
    "client.upsert(collection_name=COLLECTION, points=points)\n",
    "\n",
    "print(\"Collection populated.\")\n",
    "print(\"Count:\", client.count(collection_name=COLLECTION, exact=True).count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Nt61KFFCwlO_",
   "metadata": {
    "id": "Nt61KFFCwlO_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: chair\n",
      "Hits: 3\n",
      "Payload keys: ['filename', 'caption']\n",
      "Preview: a close up of a toilet with a pink seat and lid\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Sanity check: can we retrieve anything? (1 query only)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - run ONE text query\n",
    "# - get TOP_K hits\n",
    "# - print payload keys + a caption preview from the first hit\n",
    "#\n",
    "# This is NOT the full multi-intent stage (that's Part B).\n",
    "# This is just a basic \"does retrieval work at all?\" check.\n",
    "\n",
    "TOP_K = 3\n",
    "test_query = \"chair\"   # keep it boring on purpose\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_text_clip(text: str) -> np.ndarray:\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Note: this mirrors the image side — text features may also come back as a bundle.\n",
    "    # TRAP: feats may be a bundle (dict/object). Extract the tensor.\n",
    "    # --- YOUR CODE HERE ---\n",
    "    # Example (do not assume): feats = feats[\"pooler_output\"]\n",
    "\n",
    "    # SOLUTION (filled in):\n",
    "    # Same as image side: get_text_features() returns a bundle.\n",
    "    # Extract pooler_output for the projected (B, 512) embedding.\n",
    "    tmp = model.get_text_features(**inputs)\n",
    "    feats = tmp.pooler_output\n",
    "\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    v = feats.detach().cpu().numpy().astype(np.float32)[0]\n",
    "    return l2_normalize(v[None, :])[0].astype(np.float32)\n",
    "\n",
    "qv = embed_text_clip(test_query)\n",
    "\n",
    "# TRAP: Qdrant API may differ by version.\n",
    "# Your job: make ONE of these work.\n",
    "# Hint: inspect dir(client)\n",
    "#\n",
    "# Option A (some versions):\n",
    "# hits = client.search(\n",
    "#     collection_name=COLLECTION,\n",
    "#     query_vector=qv.tolist(),\n",
    "#     limit=TOP_K,\n",
    "#     with_payload=True,\n",
    "# )\n",
    "#\n",
    "# Option B (newer versions):\n",
    "# TODO: find the correct method name + arguments in your Qdrant version\n",
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "# Qdrant 1.16+: search() is removed, use query_points() instead\n",
    "result = client.query_points(\n",
    "    collection_name=COLLECTION,\n",
    "    query=qv.tolist(),\n",
    "    limit=TOP_K,\n",
    "    with_payload=True,\n",
    ")\n",
    "hits = result.points\n",
    "\n",
    "assert hits is not None and len(hits) > 0, \"No hits returned. Retrieval not working.\"\n",
    "\n",
    "print(\"Query:\", test_query)\n",
    "print(\"Hits:\", len(hits))\n",
    "\n",
    "h0 = hits[0]\n",
    "payload0 = getattr(h0, \"payload\", None) or {}\n",
    "print(\"Payload keys:\", list(payload0.keys()))\n",
    "\n",
    "caption_preview = (payload0.get(\"caption\") or payload0.get(\"filename\") or \"\")\n",
    "print(\"Preview:\", str(caption_preview)[:140])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n80PqWjXggl7",
   "metadata": {
    "id": "n80PqWjXggl7"
   },
   "source": [
    "# Save to Hugging Face (for Notebook B)\n",
    "\n",
    "By the end of Part A, your Hugging Face dataset repo must contain **exactly these files**:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "├── img_vecs.npy      # image embeddings (shape: N × D)\n",
    "├── payloads.json     # metadata per image (filename + caption)\n",
    "└── meta.json         # index metadata (dataset, model, caption source)\n",
    "```\n",
    "\n",
    "Use a **private** Hugging Face *dataset* repo under your own account.\n",
    "\n",
    "Notebook B will download this folder and rebuild Qdrant **without re-embedding**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7wFNkBq9DVQs",
   "metadata": {
    "id": "7wFNkBq9DVQs"
   },
   "source": [
    "## Metadata for reuse (important)\n",
    "\n",
    "Before uploading your embeddings, you must save a small `meta.json` file.\n",
    "\n",
    "This file explains:\n",
    "- what dataset you used\n",
    "- how many items are indexed\n",
    "- what model and embedding size were used\n",
    "- where captions came from in the dataset schema\n",
    "\n",
    "This is required so **Part B can rebuild Qdrant without re-embedding**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BX-ZzLVHggl7",
   "metadata": {
    "id": "BX-ZzLVHggl7"
   },
   "source": [
    "## Colab Secrets token\n",
    "\n",
    "Colab left sidebar → key icon (Secrets) → add:\n",
    "- Name: HF_TOKEN\n",
    "- Value: your HF token (Write permission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "xmBp7PNWDbxC",
   "metadata": {
    "id": "xmBp7PNWDbxC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved meta.json:\n",
      "{\n",
      "  \"num_items\": 300,\n",
      "  \"embedding_dim\": 512,\n",
      "  \"model_name\": \"openai/clip-vit-base-patch32\",\n",
      "  \"dataset_name\": \"lmms-lab/COCO-Caption2017\",\n",
      "  \"caption_source\": \"sample['answer'][0]\",\n",
      "  \"notes\": \"Add any observations about the dataset or extraction here\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Save metadata about this embedding index\n",
    "# ============================================================\n",
    "\n",
    "HF_USER_NAME = \"vector-helix\"\n",
    "HF_REPO_NAME = \"latent-assembly-clip-qdrant\"\n",
    "\n",
    "ARTIFACT_DIR = Path(HF_REPO_NAME + \"/dataset\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TODO (small but important):\n",
    "# You should already have DATASET_NAME and CAPTION_SOURCE defined earlier.\n",
    "# This metadata will be used by Part B to rebuild Qdrant without guessing.\n",
    "\n",
    "meta = {\n",
    "    \"num_items\": int(len(payloads)),\n",
    "    \"embedding_dim\": int(img_vecs.shape[1]),\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"caption_source\": CAPTION_SOURCE,\n",
    "    \"notes\": \"Add any observations about the dataset or extraction here\"\n",
    "}\n",
    "\n",
    "with open(ARTIFACT_DIR / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved meta.json:\")\n",
    "print(json.dumps(meta, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "KfaZfmEWggl7",
   "metadata": {
    "id": "KfaZfmEWggl7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290ef49506a046959d3d3d43b7af5f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e9e09e9de449f68c4718244aeb5de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to: vector-helix/latent-assembly-clip-qdrant\n"
     ]
    }
   ],
   "source": [
    "# TODO: Upload artifacts to your private HF dataset repo\n",
    "\n",
    "# huggingface_hub = ensure_package_installed(\"huggingface_hub\", \"huggingface_hub\")\n",
    "\n",
    "try:\n",
    "    # from google.colab import userdata\n",
    "    # HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "except Exception:\n",
    "    HF_TOKEN = None\n",
    "\n",
    "assert HF_TOKEN, \"HF_TOKEN not found. Set it via environment variable or Colab Secrets.\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# TODO: change to your own repo id, under your username\n",
    "# HF_REPO_ID = \"YOUR_USERNAME/YOUR_DATASET_REPO_NAME\"\n",
    "\n",
    "# SOLUTION (filled in):\n",
    "\n",
    "HF_REPO_ID = HF_USER_NAME + \"/\" + HF_REPO_NAME\n",
    "\n",
    "ARTIFACT_DIR = Path(HF_REPO_NAME + \"/dataset\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(ARTIFACT_DIR / \"img_vecs.npy\", img_vecs)\n",
    "with open(ARTIFACT_DIR / \"payloads.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payloads, f, ensure_ascii=False)\n",
    "\n",
    "# IMPORTANT: do NOT overwrite meta.json here.\n",
    "# It must include dataset_name + caption_source from the metadata cell above.\n",
    "assert (ARTIFACT_DIR / \"meta.json\").exists(), (\n",
    "    \"meta.json is missing. Run the metadata cell above before uploading.\"\n",
    ")\n",
    "\n",
    "create_repo(repo_id=HF_REPO_ID, repo_type=\"dataset\", private=True, exist_ok=True)\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=HF_REPO_ID,\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=\"latent-assembly-clip-qdrant\",\n",
    "    path_in_repo=\".\",\n",
    "    commit_message=\"Upload Latent Assembly artifacts\",\n",
    ")\n",
    "\n",
    "print(\"Uploaded to:\", HF_REPO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xciLH-Oeggl7",
   "metadata": {
    "id": "xciLH-Oeggl7"
   },
   "source": [
    "## Submit (Part A)\n",
    "\n",
    "Submit your HF repo id: `username/repo-name`\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
