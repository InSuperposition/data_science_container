{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Hooks Deep Dive: Forward and Backward Hooks on ResNet18\n",
    "\n",
    "This notebook demonstrates how to use PyTorch's hook mechanism to intercept\n",
    "forward and backward passes through a neural network. We'll register hooks\n",
    "on all convolutional layers in a pre-trained ResNet18 model.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand forward vs backward hooks\n",
    "2. Navigate nested model architectures\n",
    "3. Track layer execution order during forward/backward passes\n",
    "4. Use closures to create reusable hook functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting best available device...\n",
      "CUDA → Built: No (PyTorch compiled without CUDA support)\n",
      "MPS → Built: Yes | Available: Yes → Using mps (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def get_device_type(verbose: bool = True) -> torch.device:\n",
    "    \"\"\"\n",
    "    Returns the best available device: CUDA → MPS → CPU\n",
    "    Explicitly separates 'is_built' and 'is_available' checks for both backends.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Detecting best available device...\")\n",
    "\n",
    "    # ------------------- CUDA -------------------\n",
    "    cuda_built = torch.version.cuda is not None\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "\n",
    "    if cuda_built:\n",
    "        if cuda_available:\n",
    "            device = torch.device(\"cuda\")\n",
    "            if verbose:\n",
    "                print(f\"CUDA → Built: Yes | Available: Yes → Using {device}\")\n",
    "                print(\n",
    "                    f\"   GPU: {torch.cuda.get_device_name(0)} | Count: {torch.cuda.device_count()}\"\n",
    "                )\n",
    "            return device\n",
    "        elif verbose:\n",
    "            print(\"CUDA → Built: Yes | Available: No (driver/GPU issue)\")\n",
    "    elif verbose:\n",
    "        print(\"CUDA → Built: No (PyTorch compiled without CUDA support)\")\n",
    "\n",
    "    # ------------------- MPS (Apple Silicon) -------------------\n",
    "    mps_built = torch.backends.mps.is_built()\n",
    "    mps_available = torch.backends.mps.is_available()\n",
    "\n",
    "    if mps_built:\n",
    "        if mps_available:\n",
    "            device = torch.device(\"mps\")\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"MPS → Built: Yes | Available: Yes → Using {device} (Apple Silicon GPU)\"\n",
    "                )\n",
    "            return device\n",
    "        elif verbose:\n",
    "            print(\"MPS → Built: Yes | Available: No (macOS <12.3 or Intel Mac)\")\n",
    "    elif verbose:\n",
    "        print(\"MPS → Built: No (PyTorch compiled without MPS support)\")\n",
    "\n",
    "    # ------------------- CPU Fallback -------------------\n",
    "    device = torch.device(\"cpu\")\n",
    "    if verbose:\n",
    "        print(\"Falling back to CPU\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = get_device_type(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/tensor/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 loaded successfully!\n",
      "Model on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet18 model\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode (disables dropout, batch norm training mode, etc.)\n",
    "\n",
    "print(\"ResNet18 loaded successfully!\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding ResNet18's Nested Structure\n",
    "\n",
    "ResNet18 has a hierarchical architecture:\n",
    "- `conv1`: Initial 7x7 convolution\n",
    "- `layer1` → `layer4`: Four residual layer groups\n",
    "  - Each layer contains 2 BasicBlocks\n",
    "  - Each BasicBlock has 2 conv layers (conv1, conv2)\n",
    "  - Some blocks have downsample layers (1x1 conv for dimension matching)\n",
    "\n",
    "Let's identify all Conv2d layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Conv2d layers: 20\n",
      "\n",
      "Layer names:\n",
      "  1. conv1\n",
      "  2. layer1.0.conv1\n",
      "  3. layer1.0.conv2\n",
      "  4. layer1.1.conv1\n",
      "  5. layer1.1.conv2\n",
      "  6. layer2.0.conv1\n",
      "  7. layer2.0.conv2\n",
      "  8. layer2.0.downsample.0\n",
      "  9. layer2.1.conv1\n",
      "  10. layer2.1.conv2\n",
      "  11. layer3.0.conv1\n",
      "  12. layer3.0.conv2\n",
      "  13. layer3.0.downsample.0\n",
      "  14. layer3.1.conv1\n",
      "  15. layer3.1.conv2\n",
      "  16. layer4.0.conv1\n",
      "  17. layer4.0.conv2\n",
      "  18. layer4.0.downsample.0\n",
      "  19. layer4.1.conv1\n",
      "  20. layer4.1.conv2\n"
     ]
    }
   ],
   "source": [
    "# Discover all Conv2d layers in ResNet18\n",
    "conv_layers = []\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        conv_layers.append(name)\n",
    "\n",
    "print(f\"Total Conv2d layers: {len(conv_layers)}\\n\")\n",
    "print(\"Layer names:\")\n",
    "for i, name in enumerate(conv_layers, 1):\n",
    "    print(f\"  {i}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook Registration Strategy\n",
    "\n",
    "We'll create two types of hooks:\n",
    "\n",
    "1. **Forward Hooks**: Triggered during forward pass\n",
    "   - Signature: `hook(module, input, output)`\n",
    "   - We'll print: \"Forward: Conv layer X\"\n",
    "\n",
    "2. **Backward Hooks**: Triggered during backward pass\n",
    "   - Signature: `hook(module, grad_input, grad_output)`\n",
    "   - We'll print: \"Backward: Conv layer X\"\n",
    "\n",
    "**Key Design Decisions:**\n",
    "- Use closures to capture layer number in hook function\n",
    "- Store hook handles to prevent garbage collection\n",
    "- Number layers consistently (1-20) for clarity\n",
    "\n",
    "**Important Note on Execution Order:**\n",
    "- Forward hooks execute in the order layers are called (1 → 20)\n",
    "- Backward hooks execute in reverse order (20 → 1) because gradients flow backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered hooks on 20 Conv2d layers\n",
      "  - 20 forward hooks\n",
      "  - 20 backward hooks\n"
     ]
    }
   ],
   "source": [
    "def create_forward_hook(layer_num: int):\n",
    "    \"\"\"\n",
    "    Create a forward hook that prints the layer number.\n",
    "\n",
    "    Args:\n",
    "        layer_num: Integer identifying the convolution layer\n",
    "\n",
    "    Returns:\n",
    "        Hook function with captured layer_num in closure\n",
    "    \"\"\"\n",
    "    def hook(module, input, output):\n",
    "        print(f\"Forward: Conv layer {layer_num}\")\n",
    "    return hook\n",
    "\n",
    "\n",
    "def create_backward_hook(layer_num: int):\n",
    "    \"\"\"\n",
    "    Create a backward hook that prints the layer number.\n",
    "\n",
    "    Args:\n",
    "        layer_num: Integer identifying the convolution layer\n",
    "\n",
    "    Returns:\n",
    "        Hook function with captured layer_num in closure\n",
    "    \"\"\"\n",
    "    def hook(module, grad_input, grad_output):\n",
    "        print(f\"Backward: Conv layer {layer_num}\")\n",
    "    return hook\n",
    "\n",
    "\n",
    "# Storage for hook handles (prevents garbage collection)\n",
    "forward_hook_handles = []\n",
    "backward_hook_handles = []\n",
    "\n",
    "# Register hooks on all Conv2d layers\n",
    "layer_num = 0\n",
    "for _name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        layer_num += 1\n",
    "\n",
    "        # Register both forward and backward hooks\n",
    "        fwd_handle = module.register_forward_hook(create_forward_hook(layer_num))\n",
    "        bwd_handle = module.register_full_backward_hook(create_backward_hook(layer_num))\n",
    "\n",
    "        forward_hook_handles.append(fwd_handle)\n",
    "        backward_hook_handles.append(bwd_handle)\n",
    "\n",
    "print(f\"Registered hooks on {layer_num} Conv2d layers\")\n",
    "print(f\"  - {len(forward_hook_handles)} forward hooks\")\n",
    "print(f\"  - {len(backward_hook_handles)} backward hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input shape: torch.Size([1, 3, 224, 224])\n",
      "Requires gradient: True\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input tensor (batch_size=1, channels=3, height=224, width=224)\n",
    "# ResNet18 expects 224x224 RGB images\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=device, requires_grad=True)\n",
    "\n",
    "print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "print(f\"Requires gradient: {dummy_input.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Forward Pass\n",
    "\n",
    "This will trigger all forward hooks in execution order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FORWARD PASS\n",
      "============================================================\n",
      "Forward: Conv layer 1\n",
      "Forward: Conv layer 2\n",
      "Forward: Conv layer 3\n",
      "Forward: Conv layer 4\n",
      "Forward: Conv layer 5\n",
      "Forward: Conv layer 6\n",
      "Forward: Conv layer 7\n",
      "Forward: Conv layer 8\n",
      "Forward: Conv layer 9\n",
      "Forward: Conv layer 10\n",
      "Forward: Conv layer 11\n",
      "Forward: Conv layer 12\n",
      "Forward: Conv layer 13\n",
      "Forward: Conv layer 14\n",
      "Forward: Conv layer 15\n",
      "Forward: Conv layer 16\n",
      "Forward: Conv layer 17\n",
      "Forward: Conv layer 18\n",
      "Forward: Conv layer 19\n",
      "Forward: Conv layer 20\n",
      "============================================================\n",
      "Output shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FORWARD PASS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be [1, 1000] for ImageNet classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Dummy Loss and Run Backward Pass\n",
    "\n",
    "We'll create a simple loss by summing all outputs, then call `backward()`.\n",
    "This will trigger all backward hooks in reverse order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BACKWARD PASS\n",
      "============================================================\n",
      "Backward: Conv layer 20\n",
      "Backward: Conv layer 19\n",
      "Backward: Conv layer 18\n",
      "Backward: Conv layer 17\n",
      "Backward: Conv layer 16\n",
      "Backward: Conv layer 15\n",
      "Backward: Conv layer 14\n",
      "Backward: Conv layer 13\n",
      "Backward: Conv layer 12\n",
      "Backward: Conv layer 11\n",
      "Backward: Conv layer 10\n",
      "Backward: Conv layer 9\n",
      "Backward: Conv layer 8\n",
      "Backward: Conv layer 7\n",
      "Backward: Conv layer 6\n",
      "Backward: Conv layer 5\n",
      "Backward: Conv layer 4\n",
      "Backward: Conv layer 3\n",
      "Backward: Conv layer 2\n",
      "Backward: Conv layer 1\n",
      "============================================================\n",
      "Dummy loss value: 0.0103\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy loss (sum of all output logits)\n",
    "# In real training, you'd use a proper loss function like CrossEntropyLoss\n",
    "dummy_loss = output.sum()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BACKWARD PASS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dummy_loss.backward()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dummy loss value: {dummy_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up: Remove Hooks\n",
    "\n",
    "Good practice: Remove hooks when done to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 20 forward hooks\n",
      "Removed 20 backward hooks\n",
      "Hook cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "# Remove all hooks\n",
    "for handle in forward_hook_handles:\n",
    "    handle.remove()\n",
    "\n",
    "for handle in backward_hook_handles:\n",
    "    handle.remove()\n",
    "\n",
    "print(f\"Removed {len(forward_hook_handles)} forward hooks\")\n",
    "print(f\"Removed {len(backward_hook_handles)} backward hooks\")\n",
    "\n",
    "# Clear the lists\n",
    "forward_hook_handles.clear()\n",
    "backward_hook_handles.clear()\n",
    "\n",
    "print(\"Hook cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Exercises\n",
    "\n",
    "Now that you understand the basics, try these extensions:\n",
    "\n",
    "1. **Capture Feature Maps**: Modify forward hooks to store `output.clone()` in a dictionary\n",
    "   ```python\n",
    "   feature_maps = {}\n",
    "   def create_forward_hook(layer_num):\n",
    "       def hook(module, input, output):\n",
    "           feature_maps[f'layer_{layer_num}'] = output.clone()\n",
    "       return hook\n",
    "   ```\n",
    "\n",
    "2. **Measure Gradient Magnitudes**: Modify backward hooks to compute `grad_output[0].norm()`\n",
    "   ```python\n",
    "   def create_backward_hook(layer_num):\n",
    "       def hook(module, grad_input, grad_output):\n",
    "           grad_norm = grad_output[0].norm().item()\n",
    "           print(f\"Backward: Conv layer {layer_num} | Gradient norm: {grad_norm:.4f}\")\n",
    "       return hook\n",
    "   ```\n",
    "\n",
    "3. **Filter by Layer Type**: Register hooks only on 3x3 convolutions (exclude 1x1 and 7x7)\n",
    "   ```python\n",
    "   for name, module in model.named_modules():\n",
    "       if isinstance(module, nn.Conv2d) and module.kernel_size == (3, 3):\n",
    "           # Register hook only on 3x3 convs\n",
    "   ```\n",
    "\n",
    "4. **Timing Analysis**: Use `time.time()` in hooks to measure per-layer execution time\n",
    "\n",
    "5. **Selective Hooks**: Only hook layers in `layer3` and `layer4`\n",
    "   ```python\n",
    "   for name, module in model.named_modules():\n",
    "       if isinstance(module, nn.Conv2d) and ('layer3' in name or 'layer4' in name):\n",
    "           # Register hook\n",
    "   ```\n",
    "\n",
    "**Advanced Challenge**: Create a visualization showing which layers have the largest gradient flow during backward pass. This can help identify vanishing/exploding gradient problems in deep networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
